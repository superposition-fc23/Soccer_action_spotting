# Soccer Action Recognition - Project Architecture

---

## ğŸ¯ System Overview

**Goal**: Temporal action recognition in soccer videos (PASS, DRIVE, BACKGROUND)

**Approach**: Two-stage pipeline with frozen YOLO detector + trainable Transformer classifier

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT: Soccer Video                          â”‚
â”‚                    (25 FPS, 224p resolution)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         STAGE 1: SPATIAL DETECTION             â”‚
        â”‚    (Frozen Pre-trained YOLOv8x/YOLOv11x)      â”‚
        â”‚                                                â”‚
        â”‚  - Detect players (class 0) & ball (class 32) â”‚
        â”‚  - Extract bounding boxes & confidence scores â”‚
        â”‚  - Extract spatial features (512-dim)         â”‚
        â”‚  - Cache features to disk for efficiency      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         STAGE 2: TEMPORAL TRACKING              â”‚
        â”‚            (ByteTrack Algorithm)                â”‚
        â”‚                                                â”‚
        â”‚  - Track objects across frames (Kalman filter) â”‚
        â”‚  - Maintain track IDs, trajectories, history   â”‚
        â”‚  - Extract tracking features (128-dim)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      STAGE 3: TEMPORAL CLASSIFICATION           â”‚
        â”‚      (Trainable Transformer Encoder-Decoder)    â”‚
        â”‚                                                â”‚
        â”‚  Input: 32-frame temporal window               â”‚
        â”‚  - Combine spatial (512) + tracking (128) = 640â”‚
        â”‚  - Transformer Encoder (2 layers, 4 heads)     â”‚
        â”‚  - Hidden dim: 128                             â”‚
        â”‚  - Decoder: Temporal aggregation + classifier  â”‚
        â”‚  Output: 3 classes (PASS, DRIVE, BACKGROUND)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              OUTPUT: Action Label               â”‚
        â”‚         Softmax probabilities over 3 classes    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Detailed Component Architecture

### 1. Configuration Layer

**File**: [toy_config.py](toy_config.py)

**Purpose**: Centralized configuration for all hyperparameters

```python
Key Parameters:
â”œâ”€â”€ Dataset
â”‚   â”œâ”€â”€ MAX_TRAIN_VIDEOS = 4
â”‚   â”œâ”€â”€ MAX_VAL_VIDEOS = 2
â”‚   â”œâ”€â”€ VIDEO_RESOLUTION = "224p"
â”‚   â””â”€â”€ FPS = 25
â”œâ”€â”€ Temporal Window
â”‚   â”œâ”€â”€ TEMPORAL_WINDOW_SIZE = 32
â”‚   â”œâ”€â”€ TEMPORAL_STRIDE = 2
â”‚   â””â”€â”€ ACTION_CONTEXT_FRAMES = 8
â”œâ”€â”€ Model Architecture
â”‚   â”œâ”€â”€ FEATURE_DIM = 512 (YOLO spatial)
â”‚   â”œâ”€â”€ HIDDEN_DIM = 128 (Transformer)
â”‚   â”œâ”€â”€ NUM_LAYERS = 2 (Transformer)
â”‚   â”œâ”€â”€ NUM_HEADS = 4 (Attention)
â”‚   â””â”€â”€ DROPOUT = 0.2
â”œâ”€â”€ Training
â”‚   â”œâ”€â”€ BATCH_SIZE = 4
â”‚   â”œâ”€â”€ NUM_EPOCHS = 10
â”‚   â”œâ”€â”€ LEARNING_RATE = 5e-6
â”‚   â””â”€â”€ WEIGHT_DECAY = 1e-5
â”œâ”€â”€ Class Balancing (NEW)
â”‚   â”œâ”€â”€ USE_CLASS_BALANCING = True
â”‚   â””â”€â”€ CLASS_BALANCE_RATIOS = {0: 1.0, 1: 1.5, 2: 3.0}
â””â”€â”€ Label Mapping (13 â†’ 3 classes)
    â”œâ”€â”€ 0: PASS (includes HIGH_PASS, HEADER, CROSS)
    â”œâ”€â”€ 1: DRIVE
    â””â”€â”€ 2: BACKGROUND (includes OUT, THROW IN, SHOT, etc.)
```

---

### 2. Data Pipeline

#### 2.1 Dataset Loader

**File**: [utils/toy_dataset.py](utils/toy_dataset.py)

**Class**: `ToyActionDataset`

**Key Features**:
- Action-centric sampling around labeled actions (Â±8 frames context)
- Optional dense temporal sampling (stride=32)
- **Class balancing** via oversampling (DRIVE +50%, BACKGROUND +200%)
- Label mapping from 13 â†’ 3 classes
- Metadata tracking (gameTime, video_id, action_id)

**Data Flow**:
```
Raw Video Dataset (SoccerNet BAS-2024)
         â†“
Parse JSON annotations (labels-ball.json)
         â†“
Map 13 labels â†’ 3 classes (PASS/DRIVE/BACKGROUND)
         â†“
Create temporal windows (32 frames, stride 2)
         â†“
Apply class balancing (training only)
         â†“
Return batch: {video, label, metadata}
```

#### 2.2 Class Balancing

**Implementation**: `_apply_class_balancing()` method

**Before**:
- PASS: 52% (4,497 samples)
- DRIVE: 34% (2,942 samples)
- BACKGROUND: 14% (1,254 samples)

**After** (with ratios 1.0, 1.5, 3.0):
- PASS: ~40% (4,497 samples)
- DRIVE: ~35% (4,413 samples, +1,471)
- BACKGROUND: ~25% (3,762 samples, +2,508)

**Total**: 8,693 â†’ 12,672 samples (+3,979 duplicates)

---

### 3. Detection & Tracking Pipeline

#### 3.1 Player & Ball Detector

**File**: [models/detector.py](models/detector.py)

**Class**: `PlayerBallDetector`

**Model**: YOLOv8x or YOLOv11x (frozen, pre-trained on COCO)

**Key Operations**:
```python
detect_frame(frame, return_features=True):
    Input: RGB frame (H, W, 3)
    â†“
    Run YOLO inference
    â†“
    Filter: classes [0=person, 32=ball]
    â†“
    Apply NMS (conf=0.15, iou=0.3)
    â†“
    Extract spatial features (512-dim from backbone)
    â†“
    Output: {
        'boxes': Nx4 (x1, y1, x2, y2),
        'classes': N,
        'scores': N,
        'features': 512-dim tensor
    }
```

**Feature Caching**:
- YOLO features cached to disk: `outputs/toy_experiment/yolo_cache/`
- Cache key: `{video_id}_{frame_idx}.pt`
- Speeds up training by ~3-5x

#### 3.2 Multi-Object Tracker

**File**: [models/tracker.py](models/tracker.py)

**Class**: `ByteTracker`

**Algorithm**: ByteTrack with Kalman filtering

**Key Features**:
- Track players & ball across frames
- Assign unique track IDs
- Maintain trajectory history
- Extract tracking features:
  - Position (cx, cy)
  - Size (w, h)
  - Velocity (vx, vy)
  - Track age, hits count

**Tracking Features (128-dim)**:
```python
TrackFeatureExtractor:
    Input: List of tracks with bboxes, velocities
    â†“
    Compute statistics:
        - Ball-player distances
        - Player positions (normalized)
        - Velocities
        - Track counts
    â†“
    MLP embedding: [raw_features] â†’ 128-dim
    â†“
    Output: 128-dim track embedding per frame
```

---

### 4. Model Architecture

#### 4.1 Overall Model

**File**: [models/toy_action_classifier.py](models/toy_action_classifier.py)

**Class**: `ToyActionClassifier`

```
Input: Temporal window (32 frames)
    â”œâ”€â”€ Spatial features: (32, 512) from YOLO
    â””â”€â”€ Detections & tracks: List[Dict] per frame

        â†“

Feature Fusion:
    â”œâ”€â”€ Spatial features: (32, 512)
    â””â”€â”€ Track features: (32, 128) from TrackFeatureExtractor
        â†“
    Concatenate: (32, 640)

        â†“

Temporal Encoder (Transformer):
    â”œâ”€â”€ Input: (Batch, 32, 640)
    â”œâ”€â”€ 2 Transformer Encoder Layers
    â”œâ”€â”€ 4 Attention Heads
    â”œâ”€â”€ Hidden dim: 128
    â”œâ”€â”€ Dropout: 0.2
    â””â”€â”€ Output: (Batch, 32, 128)

        â†“

Temporal Decoder:
    â”œâ”€â”€ GRU layer (1 layer, hidden=128)
    â”œâ”€â”€ Take last hidden state: (Batch, 128)
    â””â”€â”€ Linear classifier: 128 â†’ 3 classes

        â†“

Output: (Batch, 3) logits â†’ Softmax â†’ Probabilities
```

#### 4.2 Temporal Encoder

**File**: [models/action_classifier.py](models/action_classifier.py) (imported)

**Class**: `TemporalEncoder`

**Type**: Transformer (2 layers, 4 heads)

**Architecture**:
```python
TransformerEncoder:
    â”œâ”€â”€ Positional Encoding (learned)
    â”œâ”€â”€ Layer 1:
    â”‚   â”œâ”€â”€ Multi-Head Self-Attention (4 heads)
    â”‚   â”œâ”€â”€ LayerNorm
    â”‚   â”œâ”€â”€ Feedforward (128 â†’ 512 â†’ 128)
    â”‚   â””â”€â”€ Residual connection
    â”œâ”€â”€ Layer 2: (same structure)
    â””â”€â”€ Output: Contextualized sequence (32, 128)
```

#### 4.3 Temporal Decoder (simplified to a linear layer)

**Class**: `TemporalDecoder`

**Architecture**:
```python
GRU-based decoder:
    â”œâ”€â”€ GRU: (input=128, hidden=128, 1 layer)
    â”œâ”€â”€ Take final hidden state: h_T
    â”œâ”€â”€ Dropout (0.2)
    â””â”€â”€ Linear: 128 â†’ 3 classes
```

---

### 5. Training Pipeline

#### 5.1 Training Script

**File**: [toy_train.py](toy_train.py)

**Class**: `ToyTrainer`

**Loss Function**:
- Primary: CrossEntropyLoss (weighted by class frequency)
- Optional: Focal Loss (disabled - causes NaN)

**Optimizer**:
- Adam: lr=5e-6, weight_decay=1e-5
- Warmup: 2 epochs (1e-7 â†’ 5e-6)
- Scheduler: ReduceLROnPlateau (factor=0.5, patience=7)

**Early Stopping**:
- Patience: 7 epochs
- Monitor: Validation loss

#### 5.2 Feature Extraction in Training

**Critical Dimension Handling** (Fixed):

**Location 1 - Cached Features**:
```python
# Load cached YOLO features
raw_f = torch.load(cache_path)

# Fix: Ensure at least 1D (prevent 0-dim tensors)
if not hasattr(raw_f, 'shape') or raw_f.dim() == 0:
    raw_f = raw_f.view(1) if hasattr(raw_f, 'view') else torch.tensor([raw_f])

# Project if needed
if raw_dim != FEATURE_DIM:
    f_proj = feature_proj(raw_f)
    f_proj = feature_norm(f_proj)
    frame_features.append(f_proj)
```

**Location 2 - Fresh YOLO Features**:
```python
# Extract features from YOLO
raw_features = detection_result['features']  # (C, H, W)

# Pool to (1, 1) spatial dims
feat = F.adaptive_avg_pool2d(raw_features.unsqueeze(0), (1, 1))

# Fix: Carefully remove only spatial and batch dims
feat = feat.squeeze(-1).squeeze(-1).squeeze(0)  # â†’ (C,)

# Safety check for 0-dim tensors
if feat.dim() == 0:
    feat = feat.unsqueeze(0)
```

---

### 6. Evaluation & Metrics

#### 6.1 Unified Metrics Module

**File**: [utils/metrics.py](utils/metrics.py)

**Class**: `MetricsTracker`

**Computed Metrics**:
```python
MetricsTracker.compute() returns:
    â”œâ”€â”€ Basic
    â”‚   â”œâ”€â”€ loss: Average loss
    â”‚   â””â”€â”€ accuracy: Top-1 accuracy (%)
    â”œâ”€â”€ F1 Scores
    â”‚   â”œâ”€â”€ per_class_f1: [F1_PASS, F1_DRIVE, F1_BACKGROUND]
    â”‚   â””â”€â”€ macro_f1: Mean of per-class F1
    â”œâ”€â”€ Confusion Matrix
    â”‚   â””â”€â”€ confusion_matrix: 3x3 matrix
    â”œâ”€â”€ Average Precision
    â”‚   â”œâ”€â”€ map_at_1: Mean AP at rank 1 (â‰ˆ accuracy)
    â”‚   â”œâ”€â”€ tight_avg_map: Macro-averaged AP
    â”‚   â””â”€â”€ per_class_ap: [AP_PASS, AP_DRIVE, AP_BACKGROUND]
    â”œâ”€â”€ Classification Report
    â”‚   â””â”€â”€ Precision, Recall, F1 per class (sklearn format)
    â””â”€â”€ num_samples: Total samples evaluated
```

**Visualization Methods**:
- `plot_confusion_matrix()` â†’ RGB array (for TensorBoard)
- `plot_per_class_f1()` â†’ Bar chart
- `plot_per_class_ap()` â†’ Bar chart

**Usage Pattern**:
```python
# Initialize
tracker = MetricsTracker(class_names={0: "PASS", 1: "DRIVE", 2: "BACKGROUND"})

# Accumulate during epoch
for batch in dataloader:
    outputs, labels, loss = forward_pass(batch)
    tracker.update(outputs, labels, loss)

# Compute at end
metrics = tracker.compute()
tracker.print_summary("validation")

# Log to TensorBoard
cm_img = tracker.plot_confusion_matrix()
writer.add_image('ConfusionMatrix/val', cm_img, epoch, dataformats='HWC')

# Reset for next epoch
tracker.reset()
```

#### 6.2 Fast Evaluation Script (Optional - For quick sampling)

**File**: [evaluate_model_fast.py](evaluate_model_fast.py)

**Class**: `FastModelEvaluator`

**Purpose**: Quick evaluation on subset of data

**Features**:
- Evaluate N batches only (fast mode)
- Optional time filtering (e.g., minutes 10-13 of video)
- Generates all plots (confusion matrix, F1, AP, summary)
- Saves results to JSON

**Usage**:
```bash
python evaluate_model_fast.py \
    --model outputs/toy_experiment/models/toy_best.pth \
    --output evaluation_results_fast \
    --train-batches 50 \
    --val-batches 25 \
    --time-start 10 \
    --time-end 13
```

---

### 7. Inference Pipeline

#### 7.1 Headless Inference (Video Output)

**File**: [inference_headless.py](inference_headless.py)

**Class**: `HeadlessInference`

**Purpose**: Save annotated video with predictions (no live display)

**Key Features**:
- âœ… Confidence threshold parameter (default: 0.5)
- âœ… Always show all class counts (even zeros)
- âœ… Black text for all annotations
- Sliding window classification (32 frames, stride 8)
- Frame-by-frame annotation & writing

**Usage**:
```bash
python inference_headless.py \
    --video "Toy challenge.mp4" \
    --model outputs/toy_experiment/models/toy_best.pth \
    --output output_inference.mp4 \
    --window-size 32 \
    --stride 8 \
    --confidence-threshold 0.7
```

**Annotation Format**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Action: DRIVE                   â”‚ (Black text)
â”‚                                 â”‚
â”‚ PASS: 142                       â”‚ (Black text)
â”‚ DRIVE: 87                       â”‚ (Black text)
â”‚ BACKGROUND: 23                  â”‚ (Black text)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 7.2 Dual-View Inference (Pending)

**Status**: NOT YET IMPLEMENTED

**Purpose**: Side-by-side visualization
- Left: Real-time video with predictions
- Right: 32-frame temporal window (what model sees)

---

### 8. Output & Results Structure

```
outputs/toy_experiment/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ toy_best.pth       (Best model by val loss)
â”‚   â””â”€â”€ toy_latest.pth     (Latest epoch checkpoint)
â”œâ”€â”€ results/
â”‚   â””â”€â”€ Toy_run_6_inference.mp4  (Inference video)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ toy_run_20251210_131122/
â”‚       â””â”€â”€ events.out.tfevents.*  (TensorBoard logs)
â”œâ”€â”€ visualizations/
â”‚   â””â”€â”€ (Empty - metrics not yet integrated)
â””â”€â”€ yolo_cache/
    â””â”€â”€ {video_id}_{frame_idx}.pt  (Cached YOLO features)
```

**Checkpoint Format**:
```python
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'best_val_loss': best_val_loss,
    'history': {
        'train_loss': [...],
        'val_loss': [...],
        'train_acc': [...],
        'val_acc': [...]
    }
}
```

---

## ğŸ”„ Data Flow: Training

```
1. Dataset Loading
   â”œâ”€â”€ Parse JSON annotations
   â”œâ”€â”€ Map 13 labels â†’ 3 classes
   â”œâ”€â”€ Create temporal windows (32 frames)
   â”œâ”€â”€ Apply class balancing (train only)
   â””â”€â”€ Batch: {video, label, metadata}

2. Feature Extraction (per batch)
   â”œâ”€â”€ For each video in batch (shape: B, T, C, H, W)
   â”‚   â”œâ”€â”€ For each frame in video (T=32)
   â”‚   â”‚   â”œâ”€â”€ Check cache: yolo_cache/{video_id}_{frame_idx}.pt
   â”‚   â”‚   â”œâ”€â”€ If cached:
   â”‚   â”‚   â”‚   â””â”€â”€ Load cached features (512-dim)
   â”‚   â”‚   â””â”€â”€ Else:
   â”‚   â”‚       â”œâ”€â”€ Run YOLO detection
   â”‚   â”‚       â”œâ”€â”€ Extract spatial features (512-dim)
   â”‚   â”‚       â”œâ”€â”€ Run ByteTrack tracking
   â”‚   â”‚       â””â”€â”€ Cache features to disk
   â”‚   â””â”€â”€ Extract tracking features (128-dim)
   â””â”€â”€ Combine: spatial (512) + tracking (128) = 640-dim

3. Model Forward Pass
   â”œâ”€â”€ Input: (B, 32, 640)
   â”œâ”€â”€ Transformer Encoder: (B, 32, 640) â†’ (B, 32, 128)
   â”œâ”€â”€ Linear layer: (B, 32, 128) â†’ (B, 128)
   â””â”€â”€ Classifier: (B, 128) â†’ (B, 3)

4. Loss & Optimization
   â”œâ”€â”€ CrossEntropyLoss(outputs, labels)
   â”œâ”€â”€ loss.backward()
   â””â”€â”€ optimizer.step()

5. Metrics Tracking (NOT YET INTEGRATED)
   â””â”€â”€ Will use MetricsTracker for comprehensive evaluation
```

---

## ğŸ”„ Data Flow: Inference

```
1. Load Model
   â””â”€â”€ ToyActionClassifier.load_state_dict(checkpoint)

2. Initialize Components
   â”œâ”€â”€ PlayerBallDetector (YOLOv8x)
   â””â”€â”€ ByteTracker

3. Process Video
   â”œâ”€â”€ Open video file
   â””â”€â”€ Create video writer

4. Sliding Window Classification
   â”œâ”€â”€ Read frame
   â”œâ”€â”€ Add to buffer
   â”œâ”€â”€ If buffer >= 32 frames:
   â”‚   â”œâ”€â”€ Extract features (last 32 frames)
   â”‚   â”‚   â”œâ”€â”€ YOLO detection
   â”‚   â”‚   â”œâ”€â”€ ByteTrack tracking
   â”‚   â”‚   â””â”€â”€ Combine spatial + tracking features
   â”‚   â”œâ”€â”€ Forward pass: features â†’ logits
   â”‚   â”œâ”€â”€ Softmax: logits â†’ probabilities
   â”‚   â”œâ”€â”€ Check confidence threshold
   â”‚   â”œâ”€â”€ Get prediction: argmax(probabilities)
   â”‚   â””â”€â”€ Update class counts
   â”œâ”€â”€ Annotate frame with prediction + counts
   â”œâ”€â”€ Write frame to output video
   â””â”€â”€ Slide buffer by stride (8 frames)

5. Save Output
   â””â”€â”€ Close video writer
```

---

## ğŸ¨ Label Mapping

**From 13 Original Labels â†’ 3 Final Classes**:

```python
LABEL_MAPPING_5_TO_3 = {
    # PASS class (0)
    0: 0,   # PASS â†’ PASS
    2: 0,   # HIGH_PASS â†’ PASS
    3: 0,   # HEADER â†’ PASS
    7: 0,   # CROSS â†’ PASS

    # DRIVE class (1)
    1: 1,   # DRIVE â†’ DRIVE

    # BACKGROUND class (2)
    4: 2,   # BACKGROUND â†’ BACKGROUND
    5: 2,   # OUT â†’ BACKGROUND
    6: 2,   # THROW IN â†’ BACKGROUND
    8: 2,   # BALL PLAYER BLOCK â†’ BACKGROUND
    9: 2,   # SHOT â†’ BACKGROUND
    10: 2,  # PLAYER SUCCESSFUL TACKLE â†’ BACKGROUND
    11: 2,  # FREE KICK â†’ BACKGROUND
    12: 2   # GOAL â†’ BACKGROUND
}
```

**Class Distribution** (after balancing):
- PASS: ~40% (includes tactical passes, crosses, headers)
- DRIVE: ~35% (dribbling with ball)
- BACKGROUND: ~25% (all other actions, game stoppages)

---

## ğŸ”§ Performance Optimizations

### 1. YOLO Feature Caching
- **Location**: `outputs/toy_experiment/yolo_cache/`
- **Format**: `{video_id}_{frame_idx}.pt`
- **Impact**: ~3-5x training speedup

### 2. Reduced Model Size
- Hidden dim: 256 â†’ 128
- Transformer layers: 4 â†’ 2
- Attention heads: 8 â†’ 4
- **Impact**: Faster training, less GPU memory

### 3. Class Balancing
- Oversample minority classes (DRIVE +50%, BACKGROUND +200%)
- **Impact**: Better minority class performance

### 4. Intelligent Frame Filtering (Optional)
- Filter frames by ball-player distance
- `USE_INTELLIGENT_FILTERING = True`

---

## (Toy Run 7) Training Setup**:
- Videos: 4 train, 2 val
- Epochs: 10
- Batch size: 4
- LR: 5e-6
- Class balancing: Enabled

**Note**: Specific metrics available in TensorBoard at:
```bash
tensorboard --logdir=outputs/toy_experiment/logs
```

---

## ğŸš€ Pending Improvements

### High Priority
1. âœ… Dataset class balancing - COMPLETED
2. âœ… Unified metrics module - COMPLETED
3. â³ Integrate metrics into training loop
4. âœ… Add confidence threshold to inference - COMPLETED
5. âœ… Update inference visualization (black text, all counts) - COMPLETED

### Medium Priority
6. â³ Create dual-view inference script
7. â³ Implement mAP computation in training

### Optional
- Vision Transformer exploration (Task 4 - deferred)
- Hyperparameter tuning
- Multi-GPU training support

---

## ğŸ› ï¸ Key Files Reference

### Core Models
- [models/toy_action_classifier.py](models/toy_action_classifier.py) - Main classifier
- [models/detector.py](models/detector.py) - YOLO wrapper
- [models/tracker.py](models/tracker.py) - ByteTrack implementation
- [models/action_classifier.py](models/action_classifier.py) - Encoder/Decoder components

### Data & Utils
- [toy_config.py](toy_config.py) - Configuration
- [utils/toy_dataset.py](utils/toy_dataset.py) - Dataset loader with balancing
- [utils/metrics.py](utils/metrics.py) - Unified metrics tracking
- [utils/frame_filter.py](utils/frame_filter.py) - Intelligent filtering (optional)

### Training & Evaluation
- [toy_train.py](toy_train.py) - Main training script
- [toy_train_resume.py](toy_train_resume.py) - Resume training from checkpoint
- [evaluate_model_fast.py](evaluate_model_fast.py) - Fast evaluation

### Inference
- [inference_headless.py](inference_headless.py) - Video output inference

---

**End of Architecture Document**
